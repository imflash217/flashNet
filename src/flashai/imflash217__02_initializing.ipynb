{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why we need a good `init`???\n",
    "\n",
    "To understand why a good initialization is important to Neural Nets; lets focus on the basic operation we have there in Neural networks: `matrix multiplication`. \n",
    "So lets just take a vector & a matrix and multiply them 100 folds over (as if we have a neuralNet of 100 layers)\n",
    "and then analyze the `stats` of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(512)\n",
    "w = torch.randn(512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    x = w@x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(nan), tensor(nan))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above mean & std indicates that after so many matrix multiplactions, the output becomes NaN\n",
    "### That's bad for any computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "### To actually check just exactly after how many interations the values became NaN intractable\n",
    "x = torch.randn(512)\n",
    "w = torch.randn(512, 512)\n",
    "\n",
    "for i in range(100):\n",
    "    x = w@x\n",
    "    if x.std() != x.std():\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : tensor(0.2251)\n",
      "1 : tensor(0.0501)\n",
      "2 : tensor(0.0116)\n",
      "3 : tensor(0.0026)\n",
      "4 : tensor(0.0005)\n",
      "5 : tensor(0.0001)\n",
      "6 : tensor(2.9257e-05)\n",
      "7 : tensor(6.8481e-06)\n",
      "8 : tensor(1.5942e-06)\n",
      "9 : tensor(3.8020e-07)\n",
      "10 : tensor(9.0797e-08)\n",
      "11 : tensor(1.9798e-08)\n",
      "12 : tensor(4.4196e-09)\n",
      "13 : tensor(1.0283e-09)\n",
      "14 : tensor(2.2761e-10)\n",
      "15 : tensor(4.9857e-11)\n",
      "16 : tensor(1.1381e-11)\n",
      "17 : tensor(2.6433e-12)\n",
      "18 : tensor(6.0717e-13)\n",
      "19 : tensor(1.4122e-13)\n",
      "20 : tensor(3.0361e-14)\n",
      "21 : tensor(6.9857e-15)\n",
      "22 : tensor(1.5226e-15)\n",
      "23 : tensor(3.3332e-16)\n",
      "24 : tensor(7.2685e-17)\n",
      "25 : tensor(1.6163e-17)\n",
      "26 : tensor(3.5162e-18)\n",
      "27 : tensor(7.8414e-19)\n",
      "28 : tensor(1.7999e-19)\n",
      "29 : tensor(4.0934e-20)\n",
      "30 : tensor(9.5083e-21)\n",
      "31 : tensor(2.2057e-21)\n",
      "32 : tensor(5.1064e-22)\n",
      "33 : tensor(1.1547e-22)\n",
      "34 : tensor(2.6878e-23)\n",
      "35 : tensor(6.2063e-24)\n",
      "36 : tensor(1.4418e-24)\n",
      "37 : tensor(3.5163e-25)\n",
      "38 : tensor(8.0351e-26)\n",
      "39 : tensor(1.7859e-26)\n",
      "40 : tensor(4.1614e-27)\n",
      "41 : tensor(9.6825e-28)\n",
      "42 : tensor(2.1725e-28)\n",
      "43 : tensor(5.0006e-29)\n",
      "44 : tensor(1.1897e-29)\n",
      "45 : tensor(2.6613e-30)\n",
      "46 : tensor(6.1398e-31)\n",
      "47 : tensor(1.4255e-31)\n",
      "48 : tensor(3.3135e-32)\n",
      "49 : tensor(7.6108e-33)\n",
      "50 : tensor(1.7827e-33)\n",
      "51 : tensor(4.1725e-34)\n",
      "52 : tensor(9.2452e-35)\n",
      "53 : tensor(2.2231e-35)\n",
      "54 : tensor(5.1739e-36)\n",
      "55 : tensor(1.1971e-36)\n",
      "56 : tensor(2.7048e-37)\n",
      "57 : tensor(6.2820e-38)\n",
      "58 : tensor(1.4528e-38)\n",
      "59 : tensor(3.3889e-39)\n",
      "60 : tensor(7.8828e-40)\n",
      "61 : tensor(1.8215e-40)\n",
      "62 : tensor(4.2259e-41)\n",
      "63 : tensor(9.7558e-42)\n",
      "64 : tensor(2.2435e-42)\n",
      "65 : tensor(5.2969e-43)\n",
      "66 : tensor(1.2612e-43)\n",
      "67 : tensor(3.0829e-44)\n",
      "68 : tensor(7.0065e-45)\n",
      "69 : tensor(0.)\n",
      "70 : tensor(0.)\n",
      "71 : tensor(0.)\n",
      "72 : tensor(0.)\n",
      "73 : tensor(0.)\n",
      "74 : tensor(0.)\n",
      "75 : tensor(0.)\n",
      "76 : tensor(0.)\n",
      "77 : tensor(0.)\n",
      "78 : tensor(0.)\n",
      "79 : tensor(0.)\n",
      "80 : tensor(0.)\n",
      "81 : tensor(0.)\n",
      "82 : tensor(0.)\n",
      "83 : tensor(0.)\n",
      "84 : tensor(0.)\n",
      "85 : tensor(0.)\n",
      "86 : tensor(0.)\n",
      "87 : tensor(0.)\n",
      "88 : tensor(0.)\n",
      "89 : tensor(0.)\n",
      "90 : tensor(0.)\n",
      "91 : tensor(0.)\n",
      "92 : tensor(0.)\n",
      "93 : tensor(0.)\n",
      "94 : tensor(0.)\n",
      "95 : tensor(0.)\n",
      "96 : tensor(0.)\n",
      "97 : tensor(0.)\n",
      "98 : tensor(0.)\n",
      "99 : tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "w = torch.randn(512, 512) * 0.01\n",
    "\n",
    "for i in range(100):\n",
    "    x = w@x\n",
    "    print(i,\":\", x.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Here every activation vanishes to 0. So, to avoid that problem people have come up with several strategies to initialize their weight matrices, such as:\n",
    "\n",
    "1. Use a standard-deviation that will make sure x and w@x have exactly the same scale.\n",
    "2. Use an orthogonal matrix to initialize the weights.\n",
    "   (Orthogonal metrices preserve L2 norm, thus x and w@x would have the same sum-of-squares (i.e. std))\n",
    "3. Use SPECTRAL-NORMALIZATION on weight matrix (w).\n",
    "   SpectralNorm: The spectral norm of w is the least possible value M, such that \n",
    "   torch.norm(w@x) <= M*torch.norm(x)\n",
    "   So, dividing w by M ensures that it doesn't overflow, but it can still underflow.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The magic number for scaling\n",
    "\n",
    "```\n",
    "Here we will focus on the Xavier Initialization and its diving factor (1/math.sqrt(num_in))\n",
    "where num_in is the number of inputs to the matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Xavier Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0397), tensor(1.4369))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)\n",
    "w = torch.randn(512,512) / math.sqrt(512)\n",
    "\n",
    "for i in range(100):\n",
    "    x = w@x\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044194173824159216"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Note that:\n",
    "1/math.sqrt(512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But where does this come from?\n",
    "Ans: If we remember the definition of Matrix Multiplication:\n",
    "     When we do `y = w@x` ; the coefficients of y are given by:\n",
    "     \n",
    "     y[i] = sum([c*d for c,d in zip(w[i], x)])\n",
    "     \n",
    "Now at the very beginning, our x vector has roughly a mean of roughly 0 and atandard-deviation of roughly 1. (since we picked it that way by using torch.randn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0897), tensor(1.0263))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(512)      ### has mean of roughly 0 and std 1.\n",
    "\n",
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important part of using any Initialization rule for any weight matrix:\n",
    "```\n",
    "NOTE: Almost all initialization rules are designed for inputs that have ZERO mean & UNIT std.\n",
    "So, it becomes very necessary to normalize the inputs before any matrix computation in Deep Learning.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Suppose;  mu = x.mean()\n",
    "Then;\n",
    "          std = math.sqrt(((x-mu)**2).mean())\n",
    "          \n",
    "Now if    mu = 0\n",
    "then,\n",
    "          std = math.sqrt((x**2).mean())\n",
    "          \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.053193461000919345, 511.54185974121094)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0.\n",
    "std   = 0.\n",
    "\n",
    "### repeating the experiment to calculate the mean & std based on above formula\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    w = torch.randn(512,512)\n",
    "    y = w@x\n",
    "    mean += y.mean().item()\n",
    "    std  += (y-y.mean()).pow(2).mean().item()\n",
    "    \n",
    "mean/100, std/100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "So, if you look carefully above the sqr_mean ~= 512\n",
    "That's NOT a coincidence!!!\n",
    "\n",
    "Because, when we sum 512 elementwise-product of w and x, the mean and standard deviation of the sum if 0 & 512 respecively IF w and x are IID.\n",
    "\n",
    "This is also shown below experimentaly.\n",
    "\n",
    "So, when we sum 512 numbers with mean=0 & sqr_mean=1, we get something that has mean=0 & sqr_mean=512\n",
    "Thus if we divide it by math.sqrt(512), we will get mean=0 & sqr_mean=1 i.e. (mean=0, std=1)\n",
    "\n",
    "Hence, the magic number 512 i.e. num_input_to_matrix or aka fan_in\n",
    "\n",
    "So if we scale the weight matrix with this magic number math.sqrt(fan_in), the output after matrix mutiplication will still have (ean=0, std=1) and hence we can repeat this multiplication multiple times and still the values will neither overflow or vanish!!! This is also show in the below cells.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005764692254332727, 0.9792335320372653)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0.\n",
    "sqr_mean = 0.\n",
    "\n",
    "for i in range(10000):\n",
    "    x = torch.randn(1)\n",
    "    w = torch.randn(1)\n",
    "    y = w*x\n",
    "    mean += y.mean().item()\n",
    "    sqr_mean  += y.pow(2).mean().item()\n",
    "\n",
    "mean/10000, sqr_mean/10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "The above result proves the hypothesis above\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0001835430972278118, 1.0037422168254853)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean = 0.\n",
    "sqr_mean = 0.\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    w = torch.randn(512, 512) / math.sqrt(512)\n",
    "    y = w@x\n",
    "    mean += y.mean().item()\n",
    "    sqr_mean += y.pow(2).mean().item()\n",
    "    \n",
    "mean/100, sqr_mean/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding `ReLU` in the mix. \n",
    "```\n",
    "Adding reLU` layer after matrix multiplication changes the distribution because `ReLU` kills almost half of the activation.\n",
    "\n",
    "So a suggestion would be to multiply the values by 2.\n",
    "\n",
    "Hence the effective normalization would be math.sqrt(fan_in/2)\n",
    "\n",
    "Below cells prove this experiment.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8.925373802185058, 252.6089190673828)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ReLU without normalization\n",
    "\n",
    "mean = 0.\n",
    "sqr_mean = 0.\n",
    "\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    w = torch.randn(512, 512)                   ### NO normalization of weights\n",
    "    y = w@x\n",
    "    y = (y>0).float()*y                         ### ReLU layer\n",
    "    mean += y.mean().item()\n",
    "    sqr_mean += y.pow(2).mean().item()\n",
    "    \n",
    "mean/100, sqr_mean/100                          ### mean != 0, std != 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(9.2130), tensor(14.1683))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### mean != 0, std != 1\n",
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.39894335955381394, 0.5006036931276321)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ReLU with noralization by math.sqrt(fan_in)\n",
    "\n",
    "mean = 0.\n",
    "sqr_mean = 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    w = torch.randn(512, 512) / math.sqrt(512)  ### Normalization of weights\n",
    "    y = w@x\n",
    "    y = (y>0).float()*y                         ### ReLU layer\n",
    "    mean += y.mean().item()\n",
    "    sqr_mean += y.pow(2).mean().item()\n",
    "    \n",
    "mean/100, sqr_mean/100                          ### mean != 0, std ~= 0.5 (due to ReLU, but still not 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3782), tensor(0.5611))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### mean != 0, std ~= 0.5 (due to ReLU, but still not 1.0)\n",
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5665678030252457, 1.006990024447441)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### ReLU with normalization by math.sqrt(fan_in/2)\n",
    "\n",
    "mean = 0.\n",
    "sqr_mean = 0.\n",
    "for i in range(100):\n",
    "    x = torch.randn(512)\n",
    "    w = torch.randn(512, 512) / math.sqrt(512/2)      ### Normalization of weights by math.sqrt(N/2)\n",
    "    y = w@x\n",
    "    y = (y>0).float()*y                               ### ReLU layer\n",
    "    mean += y.mean().item()\n",
    "    sqr_mean += y.pow(2).mean().item()\n",
    "\n",
    "mean/100, sqr_mean/100                                ### mean ~= 0.5(due to ReLU), std ~= 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5742), tensor(0.8035))"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### mean ~= 0.5 (due to ReLU), std ~= 1\n",
    "y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
