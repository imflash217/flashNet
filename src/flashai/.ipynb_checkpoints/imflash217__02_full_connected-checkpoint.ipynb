{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "##########################################\r\n",
      "###### This file was autogenerated. ######\r\n",
      "######### DO NOT EDIT this file. #########\r\n",
      "##########################################\r\n",
      "### file to edit: dev_nb/imflash217__01_matmul.ipynb ####\r\n",
      "\r\n",
      "from exp.nb_00_exports import *\r\n",
      "import operator\r\n",
      "\r\n",
      "def test(a, b, cmp, cname=None):\r\n",
      "    if cname is None:\r\n",
      "        cname = cmp.__name__\r\n",
      "    assert cmp(a, b), f\"{cname}:\\n{a}\\n{b}\"\r\n",
      "\r\n",
      "def test_eq(a,b):\r\n",
      "    test(a, b, operator.eq, \"==\")\r\n",
      "\r\n",
      "from pathlib import Path\r\n",
      "from IPython.core.debugger import set_trace\r\n",
      "import gzip\r\n",
      "\r\n",
      "import matplotlib as mpl\r\n",
      "import matplotlib.pyplot as plt\r\n",
      "\r\n",
      "import pickle\r\n",
      "import math\r\n",
      "import torch\r\n",
      "from fastai import datasets\r\n",
      "\r\n",
      "MNIST_URL = \"http://deeplearning.net/data/mnist/mnist.pkl\"\r\n",
      "\r\n",
      "\r\n",
      "### test whether the results from two methods are same or not\r\n",
      "### as its difficult to accurately achieve the same numbers due to floating-point operation\r\n",
      "### we will create a function to test whether two tensor (of same shape) are\r\n",
      "### APPROXIMATELY EQUAL withing some tolerance\r\n",
      "\r\n",
      "def near(a, b):\r\n",
      "    return torch.allclose(input=a, other=b, rtol=1e-3, atol=1e-5)\r\n",
      "def test_near(a, b):\r\n",
      "    return test(a=a, b=b, cmp=near)"
     ]
    }
   ],
   "source": [
    "!cat exp/nb_01_matmul.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `forward()` & `backward()` passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_01_matmul import *\n",
    "\n",
    "def get_data(url=MNIST_URL):\n",
    "    path = datasets.download_data(url, ext=\".gz\")\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "    return map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "def normalize(x, mean, std):\n",
    "    return (x-mean)/std               ## using broadcasting to normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step-1: Get the dataset.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-1: getting the datset\n",
    "x_train, y_train, x_valid, y_valid = get_data(url=MNIST_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1304), tensor(0.3073))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_train = x_train.mean()\n",
    "std_train  = x_train.std()\n",
    "\n",
    "(mean_train, std_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step-2: Normalize your dataset properly\n",
    "        * Normalize the valid/test dataset with the stats of train data only.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-2: Normalizing the train dataset\n",
    "x_train = normalize(x_train, mean_train, std_train)\n",
    "\n",
    "### Step-2: * Normalize the valid dataset with the stast of train data\n",
    "x_valid = normalize(x_valid, mean_train, std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(3.0614e-05), tensor(1.), tensor(-0.0058), tensor(0.9924))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Checking the mean & std afetr normalization\n",
    "(x_train.mean(), x_train.std(), x_valid.mean(), x_valid.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We can see above that the mean and std of the train/valid data are now around 0 & 1 respectively\n",
    "### So, we can now create a function to validate if the mean & std are within the limits \n",
    "### using our test methods from nb_01_matmul.py module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def test_near_zero(x, tol=1e-3):\n",
    "    assert x.abs() < tol, f\"Near zero: {x}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### checking the mean & std of normalized train data\n",
    "test_near_zero(x=x_train.mean())\n",
    "test_near_zero(x=(1-x_train.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Near zero: -0.005817127414047718",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a97fba2f76a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### checking the mean of noramlized valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8757a69a6ad9>\u001b[0m in \u001b[0;36mtest_near_zero\u001b[0;34m(x, tol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Near zero: {x}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Near zero: -0.005817127414047718"
     ]
    }
   ],
   "source": [
    "### checking the mean of noramlized valid data\n",
    "test_near_zero(x=x_valid.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Near zero: 0.007566630840301514",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e1222d81f93f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### checking the std of normalized valid data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-8757a69a6ad9>\u001b[0m in \u001b[0;36mtest_near_zero\u001b[0;34m(x, tol)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#export\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_near_zero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Near zero: {x}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Near zero: 0.007566630840301514"
     ]
    }
   ],
   "source": [
    "### checking the std of normalized valid data\n",
    "test_near_zero(x=(1-x_valid.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 784 tensor(10)\n"
     ]
    }
   ],
   "source": [
    "n, m = x_train.shape          ### total number of rows & cols in the train data\n",
    "c = y_train.max()+1           ### total number of label classes\n",
    "\n",
    "print(n, m, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Foundations version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic architecture:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step-3: Define the number of hidden layers and number of hidden cells in each layer\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-3: No of hidden units in 1 hidden layer\n",
    "num_hidden_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-4: Define the weight and bias matrices\n",
    "w1 = torch.randn(m, num_hidden_cells)\n",
    "b1 = torch.randn(num_hidden_cells)\n",
    "w2 = torch.randn(num_hidden_cells, 1)\n",
    "b2 = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0108), tensor(1.0004))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### this will be around (mean=0, std=1) as we sampled from torch.randn()\n",
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4142), tensor(0.9814))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### this will be around (mean=0, std=1) as we sampled from torch.randn()\n",
    "w2.mean(), w2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_near_zero(w1.mean())\n",
    "# test_near_zero(w2.mean())\n",
    "# test_near_zero(1-w1.std())\n",
    "# test_near_zero(1-w2.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_linear(inputs, weights, bias):\n",
    "    return inputs @ weights + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_out = layer_linear(inputs=x_valid, weights=w1, bias=b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.5419), tensor(28.1979))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_out.mean(), l1_out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### the mean and std are clearly not as intended to be to (0 & 1)\n",
    "### so, we need to initalize dour weights in a more proper manner to achieve zero-mean and unit-variance\n",
    "\n",
    "### WE WILL USE KAIMING-HE INITIALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### applying kaiming initialization to our weights\n",
    "w1 = torch.randn(m, num_hidden_cells)*math.sqrt(2/m)\n",
    "b1 = torch.randn(num_hidden_cells)\n",
    "w2 = torch.randn(num_hidden_cells, 1)*math.sqrt(2/num_hidden_cells)\n",
    "b2 = torch.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.5482e-06), tensor(0.0507))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0332), tensor(0.2217))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.mean(), w2.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0832), tensor(1.8792))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### we will now use these weights in linear layer and see the mean, std of outputs\n",
    "l1_out_kaiming = layer_linear(inputs=x_valid, weights=w1, bias=b1)\n",
    "l1_out_kaiming.mean(), l1_out_kaiming.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-1.2827), tensor(1.6462))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_out_kaiming = layer_linear(inputs=l1_out_kaiming, weights=w2, bias=b2)\n",
    "l2_out_kaiming.mean(), l2_out_kaiming.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Step-4: Defining the activations function ReLU\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining ReLU activation function\n",
    "def relu(inputs:torch.Tensor):\n",
    "    return inputs.clamp_min(0.)\n",
    "\n",
    "def relu_improved(inputs:torch.Tensor):\n",
    "    return inputs.clamp_min(0.) - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7883), tensor(1.1597))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_out = relu(layer_linear(inputs=x_valid, weights=w1, bias=b1))\n",
    "\n",
    "l1_out.mean(), l1_out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.0744), tensor(0.9116))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_out = relu(layer_linear(inputs=l1_out, weights=w2, bias=b2))\n",
    "\n",
    "l2_out.mean(), l2_out.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2883), tensor(1.1597))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1_imp = relu_improved(layer_linear(inputs=x_valid, weights=w1, bias=b1))\n",
    "l1_imp.mean(), l1_imp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0134), tensor(0.6988))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_imp = relu_improved(layer_linear(inputs=l1_imp, weights=w2, bias=b2))\n",
    "l2_imp.mean(), l2_imp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As, till now we have implemented kaiming init ourselves\n",
    "### We can now use the init module from pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.nn import init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using kaiming normal initialization from pytorch\n",
    "w1 = torch.randn(m, num_hidden_cells)\n",
    "b1 = torch.randn(num_hidden_cells)\n",
    "init.kaiming_normal_(tensor=w1, mode=\"fan_out\")\n",
    "\n",
    "t = relu_improved(layer_linear(inputs=x_valid, weights=w1, bias=b1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0002), tensor(0.0509))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.mean(), w1.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1764), tensor(1.0214))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.mean(), t.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([784, 50])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 784])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Linear(in_features=m, out_features=num_hidden_cells).weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you look closely you will see that the weight matrix used by pytorch is transposed\n",
    "### So, that's the reason for using \"fan_out\" in kaiming init method\n",
    "\n",
    "### How this is taken care of in the linear layer's matrix computation??\n",
    "### In pytorch's linear layer, before matmul they transpose the weight matrix, thus validating the shapes for matrix computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.Linear.reset_parameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.modules.conv._ConvNd.reset_parameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb):\n",
    "    layer1 = layer_linear(inputs=xb, weights=w1, bias=b1)\n",
    "    layer2 = relu_improved(layer1)\n",
    "    layer3 = layer_linear(inputs=layer2, weights=w2, bias=b2)\n",
    "    return layer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.84 ms ± 2.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 _=model(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(x_valid).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_valid.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model(x_valid).shape == torch.Size([x_valid.shape[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function: `mse()` : mean square function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def mse(preds, target):\n",
    "    \"\"\"Mean Square Error loss\"\"\"\n",
    "    return (preds.squeeze(-1)-target).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.float()\n",
    "y_valid = y_valid.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1]), torch.Size([50000]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_train = model(x_train)\n",
    "preds_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10000, 1]), torch.Size([10000]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_valid = model(x_valid)\n",
    "preds_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(18.2649), tensor(18.2924))"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_train = mse(preds=preds_train, target=y_train)\n",
    "loss_valid = mse(preds=preds_valid, target=y_valid)\n",
    "\n",
    "loss_train, loss_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients and backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_grad(inp, targ):\n",
    "    \"\"\"\n",
    "    Gradient of loss w.r.t output of the previous layer\n",
    "    There are no learning parameters in MSE, so we need to find grad only wrt input'\n",
    "    \n",
    "        mse = (1/N)((y-y_hat)**2)\n",
    "    so, mse_grad = 2*(1/N)(y-y_hat)    \n",
    "    \"\"\"\n",
    "    inp.grad = 2. * (inp.squeeze(-1) - targ).unsqueeze(-1) / inp.shape[0]\n",
    "    \n",
    "    \n",
    "def relu_grad(inp, out):\n",
    "    \"\"\"\n",
    "    ReLU doesn't have any learnable parameters;\n",
    "    so we need to find the gradient of output wrt input\n",
    "    \n",
    "        relu(x) = 0 if x < 0 else x\n",
    "    so, relu_grad_x = 0 if x < 0 else 1\n",
    "    \n",
    "    NOTE: We need to also accomodate for the upstream gradient\n",
    "    So, inp.grad = relu_grad_x * out.grad\n",
    "    \"\"\"\n",
    "    inp.grad = (inp>0).float() * out.grad\n",
    "    \n",
    "    \n",
    "def linear_grad(inp, out, weights, bias):\n",
    "    \"\"\"\n",
    "    Linear layer has \"weights\" and \"bias\" as trainable parameters;\n",
    "    so we need to find the gradients of the ouput wrt input, \"weights\" and \"bias\".\n",
    "    \n",
    "    linear(x, w, b) = y = x@w + b\n",
    "    \n",
    "    linear_grad_x = w\n",
    "    linear_grad_w = x\n",
    "    linear_grad_b = 1\n",
    "    \n",
    "    NOTE: x, w, b are matrices/vectors\n",
    "    NOTE: Remember to account for the upstream gradients\n",
    "    \"\"\"\n",
    "    inp.grad     = out.grad @ (weights.t())\n",
    "    weights.grad = (inp.unsqueeze(-1) * out.grad.unsqueeze(1)).sum(dim=0)\n",
    "    bias.grad    = out.grad.sum(dim=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_and_backward(inp, targ):\n",
    "    ### forward pass for 2-linear_layer model\n",
    "    l1   = inp @ w1 + b1\n",
    "    l2   = relu(l1)\n",
    "    out  = l2 @ w2 + b2\n",
    "    \n",
    "    loss = mse(out, targ)        ### NOTE: The actual value of loss is not used in backward pass\n",
    "    \n",
    "    ### backward pass\n",
    "    ### call the backward passes in reveresed order of layers\n",
    "    \n",
    "    mse_grad(inp=out, targ=targ)                        ### loss layer\n",
    "    linear_grad(inp=l2, out=out, weights=w2, bias=b2)   ### 2nd linear layer\n",
    "    relu_grad(inp=l1, out=l2)                           ### relu layer\n",
    "    linear_grad(inp=inp, out=l1, weights=w1, bias=b1)   ### 1st linear layer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_and_backward(inp=x_valid, targ=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "### saving the gradients wrt inputs, weights & bias to test later against pytorch autograd module\n",
    "inp_g = x_valid.grad.clone()\n",
    "w1_g  = w1.grad.clone()\n",
    "b1_g  = b1.grad.clone()\n",
    "w2_g  = w2.grad.clone()\n",
    "b2_g  = b2.grad.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50]), torch.Size([50]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1_g.shape, b1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1]), torch.Size([1]))"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2_g.shape, b2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "### using pytorch autograd to calculate our gradients wrt inputs, weights & bias\n",
    "inp_pt = x_valid.clone().requires_grad_(True)\n",
    "w1_pt  = w1.clone().requires_grad_(True)\n",
    "b1_pt  = b1.clone().requires_grad_(True)\n",
    "w2_pt  = w2.clone().requires_grad_(True)\n",
    "b2_pt  = b2.clone().requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inp, targ):\n",
    "    l1 = inp @ w1_pt + b1_pt\n",
    "    l2 = relu(l1)\n",
    "    out = l2 @ w2_pt + b2_pt\n",
    "    \n",
    "    loss = mse(out, targ)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = forward(inp=inp_pt, targ=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_near(inp_g, inp_pt.grad)\n",
    "test_near(w1_g, w1_pt.grad)\n",
    "test_near(b1_g, b1_pt.grad)\n",
    "test_near(w2_g, w2_pt.grad)\n",
    "test_near(b2_g, b2_pt.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "### As all above test cases passed....So we are doing almost as good as Pytorch\n",
    "### SO NOW WE NEED TO re-FACTOR our implementataion for effieciency and usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Factor :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type-1 re-factoring: \n",
    "```\n",
    "LAYERS as CLASSES\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU():\n",
    "    \"\"\"forward and backard passes for ReLU layer\"\"\"\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        self.out = inp.clamp_min(0.) - 0.5      ### improved relu\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.grad = (self.inp > 0).float() * self.out.grad\n",
    "        \n",
    "        \n",
    "class Linear():\n",
    "    \"\"\"forward and backward passes for linear layer\"\"\"\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias    = bias\n",
    "        \n",
    "    def __call__(self, inp):\n",
    "        self.inp     = inp\n",
    "        self.out     = self.inp @ self.weights + self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.grad     = (self.out.grad) @ self.weights.t()\n",
    "        self.weights.grad = (self.inp.unsqueeze(-1) * self.out.grad.unsqueeze(1)).sum(dim=0)\n",
    "        self.bias.grad    = (self.out.grad.sum(dim=0))\n",
    "        \n",
    "    \n",
    "class MSE():\n",
    "    \"\"\"forward and backward passes for MSE loss\"\"\"\n",
    "    def __call__(self, inp, targ):\n",
    "        self.inp  = inp\n",
    "        self.targ = targ\n",
    "        self.out  = (self.inp.squeeze() - self.targ).pow(2).mean()\n",
    "        return self.out\n",
    "    \n",
    "    def backward(self):\n",
    "        self.inp.grad = 2. * (self.inp.squeeze() - self.targ).unsqueeze(-1) / (self.inp.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "### now we have defined our classes for needed layers and parts\n",
    "### we will build our model for 2-layer fully connected MSE classifier\n",
    "\n",
    "class Model():\n",
    "    \"\"\"a 2 layer fully connected network with relu & mse-loss\"\"\"\n",
    "    def __init__(self, w1, b1, w2, b2):\n",
    "        self.layers = [Linear(weights=w1, bias=b1), ReLU(), Linear(weights=w2, bias=b2)]\n",
    "        self.loss = MSE()\n",
    "        \n",
    "    def __call__(self, inp, targ):\n",
    "        for layer in self.layers:\n",
    "            inp = layer(inp)\n",
    "        return self.loss(inp=inp, targ=targ)\n",
    "    \n",
    "    def backward(self):\n",
    "        self.loss.backward()                       ### calling the backward pass on loss layer (as it is not included in self.layers)\n",
    "        for layer in reversed(self.layers):        ### calling the backward for each layer in reversed order\n",
    "            layer.backward()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "### resetting the gradients before calculating it; so that it can be properly tested\n",
    "w1.grad, b1.grad, w2.grad, b2.grad = [None]*4\n",
    "\n",
    "model = Model(w1=w1, b1=b1, w2=w2, b2=b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 37.1 ms, sys: 667 µs, total: 37.7 ms\n",
      "Wall time: 18.7 ms\n"
     ]
    }
   ],
   "source": [
    "### calling the FORWARD PASS\n",
    "%time loss = model(inp=x_valid, targ=y_valid)\n",
    "# %timeit -n 10 loss = model(inp=x_valid, targ=y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 636 ms, total: 2.23 s\n",
      "Wall time: 1.11 s\n"
     ]
    }
   ],
   "source": [
    "### calling the BACKWARD PASS\n",
    "%time model.backward()\n",
    "# %timeit -n 10 model.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "near:\ntensor([[-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        ...,\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084]])\ntensor([[-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        ...,\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117]])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-79cccd22d1e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test_near(inp_g, x_valid.grad)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_near\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest_near\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb1_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtest_near\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtest_near\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb2_g\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/Study/Projects/flashai/flashNet/src/flashai/exp/nb_01_matmul.py\u001b[0m in \u001b[0;36mtest_near\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_near\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Study/Projects/flashai/flashNet/src/flashai/exp/nb_01_matmul.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(a, b, cmp, cname)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mcname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mcmp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{cname}:\\n{a}\\n{b}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtest_eq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: near:\ntensor([[-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        ...,\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084],\n        [-0.2645,  0.0257,  0.2722,  ...,  0.7142,  0.0177,  0.0084]])\ntensor([[-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        ...,\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117],\n        [-0.3648,  0.0336,  0.3678,  ...,  0.9847,  0.0243,  0.0117]])"
     ]
    }
   ],
   "source": [
    "# test_near(inp_g, x_valid.grad)\n",
    "test_near(w1_g, w1.grad)\n",
    "test_near(b1_g, b1.grad)\n",
    "test_near(w2_g, w2.grad)\n",
    "test_near(b2_g, b2.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
