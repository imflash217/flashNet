{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does `nn.Conv2D` init work well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_02_full_connected import *\n",
    "\n",
    "def get_data(url=MNIST_URL):\n",
    "    path = datasets.download_data(url=url, ext=\".gz\")\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\n",
    "    return map(torch.tensor, (x_train, y_train, x_valid, y_valid))\n",
    "\n",
    "def normalize(inp, mean, std):\n",
    "    return (inp-mean)/std\n",
    "\n",
    "def stats(inp):\n",
    "    return inp.mean(), inp.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.modules.conv._ConvNd.reset_parameters??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-1: Get data\n",
    "x_train, y_train, x_valid, y_valid = get_data(url=MNIST_URL)\n",
    "\n",
    "### Step-2: Normalize your data accurately\n",
    "mean_train = x_train.mean()\n",
    "std_train  = x_train.std()\n",
    "x_train = normalize(inp=x_train, mean=mean_train, std=std_train)\n",
    "x_valid = normalize(inp=x_valid, mean=mean_train, std=std_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 784]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 784]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-3: reshape your image data in a matrix format using \"view_as()\"\n",
    "x_train = x_train.view(-1, 1, 28, 28)\n",
    "x_valid = x_valid.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 1, 28, 28]),\n",
       " torch.Size([50000]),\n",
       " torch.Size([10000, 1, 28, 28]),\n",
       " torch.Size([10000]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_valid.shape, y_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step-4: Gather all the needed training params as vars\n",
    "num_train, *_ = x_train.shape\n",
    "num_c = y_train.max() + 1\n",
    "num_hidden = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, tensor(10))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(num_train, num_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.nn.Conv2d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.nn.Conv2d(in_channels=1, out_channels=num_hidden, kernel_size=(5,5), bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x_valid[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 28, 28])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 5, 5])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0608), tensor(0.9159))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### the stats should be about (0 mean, 1 variance) if its normalized\n",
    "stats(inp=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0057, grad_fn=<MeanBackward0>),\n",
       " tensor(0.1155, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(layer1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0075, grad_fn=<MeanBackward0>),\n",
       " tensor(0.1174, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(layer1.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BUT, the stats of weight & bias are not (0 mean, 1 variance)\n",
    "### That's not good. So we need to initalize our weight and bias in a better way to have zero-mean-unit-var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0167, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6492, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### lets also check the stats of the output of layer1\n",
    "out1 = layer1(x)\n",
    "stats(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0022, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0853, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init.kaiming_normal_(tensor=layer1.weight, a=1.)         ### inplace initialization\n",
    "stats(layer1(x))\n",
    "\n",
    "### here we'll notice that after we do kaiming_normal weight init, the stats of the layer output improves\n",
    "### which shows the importance of proper weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "### to experiment let's create a function that gives the output of a linear layer & relu \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def f1(inp, a=0.):\n",
    "    return F.leaky_relu(layer1(inp), negative_slope=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4895, grad_fn=<MeanBackward0>),\n",
       " tensor(0.8645, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Case-1: using kaiming initialization with ReLU\n",
    "layer1 = torch.nn.Conv2d(in_channels=1, out_channels=num_hidden, kernel_size=(5,5), bias=True)\n",
    "init.kaiming_normal_(layer1.weight, a=0.)\n",
    "out1 = f1(x)\n",
    "stats(out1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1782, grad_fn=<MeanBackward0>),\n",
       " tensor(0.3726, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Case-2: NO kaiming initialization \n",
    "layer1 = torch.nn.Conv2d(in_channels=1, out_channels=num_hidden, kernel_size=(5,5), bias=True)\n",
    "out2 = f1(x)      ### remmeber this will use the new layer1 defined above\n",
    "stats(out2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### The above results are not good. It shows that the importance of kaiming initialization \n",
    "### when ReLU layers are involved.\n",
    "\n",
    "### In Case-1: kaiming_normal_ init & a=0 ReLU  -> mean ~= 0, var ~= 1 => GOOD performance\n",
    "### In Case-2: random init          & a=0 ReLU  -> mean != 0, var != 1 => BAD performance\n",
    "\n",
    "### This above experiment demonstrates the importance of KAIMING initialization when ReLU layer are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.3575, grad_fn=<MeanBackward0>),\n",
       " tensor(0.6456, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Case-3\n",
    "layer1 = torch.nn.Conv2d(in_channels=1, out_channels=num_hidden, kernel_size=(5,5), bias=True)\n",
    "init.kaiming_normal_(layer1.weight, a=1.)    ### essentially NOT using ReLU (since a=1.)\n",
    "out3 = f1(x)\n",
    "stats(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing the Kaiming Initialization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receptive Field Size (`rec_fs`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_filters, num_inp, *_ = layer1.weight.shape\n",
    "\n",
    "rec_fs = layer1.weight[0,0].numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_filters, num_inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "fan_in  = num_inp     * rec_fs\n",
    "fan_out = num_filters * rec_fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 800)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain(a):\n",
    "    return math.sqrt(2.0/(1.0 + a**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0,\n",
       " 1.4142135623730951,\n",
       " 1.4071950894605838,\n",
       " 1.4141428569978354,\n",
       " 0.5773502691896257)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain(1), gain(0), gain(0.1), gain(0.01), gain(math.sqrt(5.)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5773502691896257, 0.5773502691896258)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gain(math.sqrt(5.)), 1/math.sqrt(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Defining the updated Kaiming Init with math.sqrt(5) factor\n",
    "\n",
    "def kaiming2(params, a, use_fan_out=False):\n",
    "    num_f, num_i, *_ = params.shape\n",
    "    rec_fs = params[0,0].numel()\n",
    "    fan = num_f * rec_fs if use_fan_out else num_i * rec_fs\n",
    "    std = gain(a) / math.sqrt(fan)\n",
    "    bound = std * math.sqrt(3.)\n",
    "    params.data.uniform_(-bound, bound)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4749, grad_fn=<MeanBackward0>),\n",
       " tensor(0.8379, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming2(layer1.weight, a=0)\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.1973, grad_fn=<MeanBackward0>),\n",
       " tensor(0.3415, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaiming2(layer1.weight, a=math.sqrt(5.))\n",
    "stats(f1(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(torch.nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Conv2d(in_channels=1, out_channels=8, kernel_size=5, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=2, padding=2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Conv2d(in_channels=32, out_channels=1, kernel_size=3, stride=2, padding=1),\n",
    "    torch.nn.AdaptiveAvgPool2d(1),\n",
    "    Flatten(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 1, 28, 28]), torch.Size([100]))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x_valid[:100]\n",
    "y = y_valid[:100].float()\n",
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0307, grad_fn=<MeanBackward0>),\n",
       " tensor(0.0030, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "stats(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = mse(preds=out, target=y)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.0004), tensor(0.0272))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats(model[0].weight.grad)\n",
    "\n",
    "### This above results shows that both the weights and gradients of weights have very bad mean, std without\n",
    "### Kaiming Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "### So, lets formulate a way to include Kaiming Initialization inside conv layers\n",
    "\n",
    "for layer in model:\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        init.kaiming_normal_(layer.weight)\n",
    "        layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.2093, grad_fn=<MeanBackward0>),\n",
       " tensor(0.1644, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model(x)\n",
    "stats(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.2714), tensor(1.1720))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mse(preds=out, target=y)\n",
    "loss.backward()\n",
    "stats(model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This above cell shows that the inclusion of Kaiming Init leads to better mean & std even for gradients\n",
    "### which is a highly necessary factor in proper training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted imflash217__02_why_sqrt5.ipynb to exp/nb_02_why_sqrt5.py\r\n"
     ]
    }
   ],
   "source": [
    "!python notebook_to_script.py imflash217__02_why_sqrt5.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
